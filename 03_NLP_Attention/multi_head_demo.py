import torch
import torch.nn as nn
import torch.nn.functional as F
from attention_demo import SelfAttention

class MultiHeadAttention(nn.Module):
    def __init__(self, embed_size,num_heads):
        super().__init__()
        self.num_heads=num_heads
        self.head_size=embed_size//num_heads
        self.heads=nn.ModuleList([
            SelfAttention(embed_size,self.head_size) for _ in range(num_heads)
        ])
        self.proj=nn.Linear(embed_size,embed_size)

    def forward(self,x):
        head_outputs=[head(x)[0] for head in self.heads]
        out=torch.cat(head_outputs,dim=-1)
        out=self.proj(out)
        return out


class FeedForward(nn.Module):
    def __init__(self, embed_size):
        super().__init__()
        self.net=nn.Sequential(
            nn.Linear(embed_size,4*embed_size),
            nn.ReLU(),
            nn.Linear(4*embed_size,embed_size)
        )

    def forward(self,x):
        return self.net(x)


class Block(nn.Module):
    def __init__(self, embed_size,num_heads):
        super().__init__()
        self.sa=MultiHeadAttention(embed_size,num_heads)
        self.ffwd=FeedForward(embed_size)
        self.ln1=nn.LayerNorm(embed_size)
        self.ln2=nn.LayerNorm(embed_size)

    def forward(self,x):
        x=x+self.sa(self.ln1(x))
        x=x+self.ffwd(self.ln2(x))
        return x

if __name__=="__main__":
    torch.manual_seed(42)
    embed_size=32
    num_heads=4
    x=torch.randn(1,6,embed_size)
    block=Block(embed_size,num_heads)
    output=block(x)


    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")

    if x.shape == output.shape:
        print("âœ… Block æ„å»ºæˆåŠŸï¼è¾“å…¥è¾“å‡ºç»´åº¦ä¸€è‡´ï¼Œå¯ä»¥æ— é™å †å ï¼")
        print("ğŸ‰ æ­å–œï¼ä½ å·²ç»äº²æ‰‹é€ å‡ºäº† Transformer çš„å¿ƒè„ï¼")
    else:
        print("âŒ ç»´åº¦ä¸åŒ¹é…ï¼Œæ£€æŸ¥ä¸€ä¸‹ Linear å±‚çš„è¾“å…¥è¾“å‡ºå¤§å°ï¼")